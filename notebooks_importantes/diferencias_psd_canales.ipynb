{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ffcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Signals\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftshift\n",
    "from scipy.signal import welch\n",
    "import scipy.signal\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)  \n",
    "\n",
    "\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "\n",
    "import auxlib; importlib.reload(auxlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb40d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_labels(header):\t\n",
    "\tt0_labels = []\n",
    "\tt1_labels = []\n",
    "\tt2_labels = []\n",
    "\tfor task in range(len(header['annotations'])):\n",
    "\t\tif header['annotations'][task][2] == 'T1':\n",
    "\t\t\tt1_labels.append(header['annotations'][task][0])\n",
    "\t\telif header['annotations'][task][2] == 'T2':\n",
    "\t\t\tt2_labels.append(header['annotations'][task][0])\n",
    "\t\telse:\n",
    "\t\t\tt0_labels.append(header['annotations'][task][0])\n",
    "\treturn t0_labels, t1_labels, t2_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd_percentage(x, fs, nfft):\n",
    "    \n",
    "    N = nfft\n",
    "    X = abs(fft(x, n=nfft))/N\n",
    "    Pxx = X**2\n",
    "    X = X[0:int(N/2)+1]\n",
    "    X[1:int(N/2)] = 2*X[1:int(N/2)] # duplicate except 0\n",
    "    \n",
    "    Pxx = Pxx[0:int(N/2)+1]\n",
    "    \n",
    "    Pxx[1:int(N/2)] = 2*Pxx[1:int(N/2)]\n",
    "\n",
    "    #Psd = (1/fs*N)*abs(fft(x, n=nfft))**2\n",
    "    #Psd = Psd[0:int(N/2)+1]\n",
    "    #Psd[1:int(N/2)] = 2*Psd[1:int(N/2)] # duplicate except 0\n",
    "    \n",
    "    f, S = scipy.signal.periodogram(x, fs,  nfft=nfft, scaling='spectrum')\n",
    "\n",
    "    #psd_sum = sum(Psd)\n",
    "    \n",
    "    #Psd_per= Psd/psd_sum\n",
    "    \n",
    "    #Psd_per= (nfft/fs)*(Psd/psd_sum)\n",
    "    \n",
    "    #f = (fs/N)*np.arange(0, int(N/2)+1, 1) = S\n",
    "    \n",
    "    Psd_per = S\n",
    "    \n",
    "    psd_sum = sum(Psd_per)\n",
    "    \n",
    "    Psd_per= Psd_per/psd_sum\n",
    "    \n",
    "    return Psd_per, Pxx, X, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc009141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_filter(data, f0, Q, fs):\n",
    "    # IIR notch filter using signal.iirnotch\n",
    "    b, a = signal.iirnotch(f0, Q, fs)\n",
    "\n",
    "    # Compute magnitude response of the designed filter\n",
    "    freq, h = signal.freqz(b, a, fs=fs)\n",
    "\n",
    "    return signal.filtfilt(b, a, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_highpass_filtering(data_channel, order, f_cutoff, fs):\n",
    "    \n",
    "    butter_filter = signal.butter(N=order, Wn=f_cutoff, output='sos', fs=fs, btype='highpass')\n",
    "    \n",
    "    high_pass_filtered_eeg = signal.sosfiltfilt(butter_filter, data_channel)\n",
    "\n",
    "    return high_pass_filtered_eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising_notch(session, task, f0, Q, fs):\n",
    "    \n",
    "    data, signal_headers, header = auxlib.loadEEG(subject=session, record=task)\n",
    "    t0_labels, t1_labels, t2_labels = task_labels(header)\n",
    "    time = np.linspace(0, len(data[0])/160, len(data[0]))\n",
    "            \n",
    "    # filtering noise\n",
    "    for i in range(0, data.shape[0]):\n",
    "        data[i] = notch_filter(data[i], f0=60, Q=30, fs=160)\n",
    "                \n",
    "        # Save Data\n",
    "        #data_t = np.transpose(data)\n",
    "        #pd.DataFrame(data_t, ).to_csv(f'media/S{session:03d}R{run:02d}_denoised_{f0}_Hz.csv')\n",
    "    \n",
    "    return data, t0_labels, t1_labels, t2_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_fragmentation(data, channel, t0_labels, t1_labels, t2_labels, t_left, t_right):\n",
    "    data_t0 = []\n",
    "    data_t1 = []\n",
    "    data_t2 = []\n",
    "    \n",
    "    for i in range(len(t0_labels)):\n",
    "        t0_index = int(t0_labels[i]*160)\n",
    "        delta = int(4.1*160)\n",
    "        t0_segment = data[channel, t0_index: t0_index+delta]\n",
    "        data_t0.append(t0_segment)\n",
    "    data_t0 = np.array(data_t0)\n",
    "    \n",
    "    \n",
    "    for j in range(len(t1_labels)):\n",
    "        t1_index = int(t1_labels[j]*160)\n",
    "        delta_left = int(t_left*160)\n",
    "        delta_right = int(t_right*160)\n",
    "        t1_segment = data[channel, t1_index-delta_left: t1_index+delta_right]\n",
    "        data_t1.append(t1_segment)\n",
    "    data_t1 = np.array(data_t1)\n",
    "    \n",
    "    for k in range(len(t2_labels)):\n",
    "        t2_index  = int(t2_labels[k]*160)\n",
    "        delta_left = int(t_left*160)\n",
    "        delta_right = int(t_right*160)\n",
    "        t2_segment = data[channel, t2_index-delta_left: t2_index+delta_right]\n",
    "        data_t2.append(t2_segment)\n",
    "    data_t2 = np.array(data_t2)\n",
    "    \n",
    "    return data_t0, data_t1, data_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments_average(data_segments, fs, nfft):\n",
    "    Np = data_segments.shape[0] \n",
    "    Psd_per_average, Pxx, X_aver, f = psd_percentage(data_segments[0], fs, nfft)\n",
    "    for i in range(Np-1):\n",
    "        Psd_per, Pxx, X_aver_n, f = psd_percentage(data_segments[i+1], fs, nfft)\n",
    "        Psd_per_average = Psd_per_average + Psd_per\n",
    "        X_aver = X_aver + X_aver_n\n",
    "    return Psd_per_average/Np, X_aver/Np , f\n",
    "\n",
    "\n",
    "def psd_per_channels(data, t0_labels, t1_labels, t2_labels, fs, nfft):\n",
    "        Psd_prom_t0_per_channel = []\n",
    "        Psd_prom_t1_per_channel = []\n",
    "        Psd_prom_t2_per_channel = []\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            data_t0, data_t1, data_t2 = task_fragmentation(data=data,channel=i, \n",
    "                                                        t0_labels=t0_labels,\n",
    "                                                        t1_labels=t1_labels, \n",
    "                                                        t2_labels=t2_labels, t_left=t_left, t_right=t_right)\n",
    "            Psd_prom_t0_channel, X_prom_t0_channel, f = segments_average(data_t0, fs=160, nfft=nfft)\n",
    "            Psd_prom_t1_channel, X_prom_t1_channel, f = segments_average(data_t1, fs=160, nfft=nfft)\n",
    "            Psd_prom_t2_channel, X_prom_t2_channel, f = segments_average(data_t2, fs=160, nfft=nfft)\n",
    "            #Adding psd_prom for each channel for each t0, t1, t2\n",
    "            Psd_prom_t0_per_channel.append(Psd_prom_t0_channel)\n",
    "            Psd_prom_t1_per_channel.append(Psd_prom_t1_channel)\n",
    "            Psd_prom_t2_per_channel.append(Psd_prom_t2_channel)\n",
    "    \n",
    "        Psd_prom_t0_per_channel = np.array(Psd_prom_t0_per_channel)\n",
    "        Psd_prom_t1_per_channel = np.array(Psd_prom_t1_per_channel)\n",
    "        Psd_prom_t2_per_channel = np.array(Psd_prom_t2_per_channel)\n",
    "    \n",
    "        return Psd_prom_t0_per_channel, Psd_prom_t1_per_channel, Psd_prom_t2_per_channel, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c408b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9d99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82863cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1938ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3422442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2edec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
